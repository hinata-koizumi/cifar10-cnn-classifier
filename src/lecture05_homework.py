# -*- coding: utf-8 -*-
"""lecture05_homework.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XMsqVBGiZY1KaZ1GaIlSi_Mp7iyY1Pvy

# 第5回講義 宿題

## 課題

今Lessonで学んだことに工夫を加えて，CNNでより高精度なCIFAR10の分類器を実装してみましょう．

### 目標値

Accuracy 76%

### ルール


- 訓練データは`x_train`， `t_train`，テストデータは`x_test`で与えられます．
- 予測ラベルは one_hot表現ではなく0~9のクラスラベル で表してください．
- **下のセルで指定されている`x_train`，`t_train`以外の学習データは使わないでください．**
- Pytorchを利用して構いません．
- 今回から基本的にAPI制限はありません．
- ただしCNNベースでないモデル（Vision Transformerなど）やtorchvision等の既存モデル，学習済みモデルは用いないでください．

### 提出方法
- 2つのファイルを提出していただきます．
    1. テストデータ (`x_test`) に対する予測ラベルをcsv形式で保存し，**Omnicampusの宿題タブから「第5回 畳み込みニューラルネットワーク」を選択して**提出してください．
    2. それに対応するpythonのコードを　ファイル＞ダウンロード＞.pyをダウンロード　から保存し，**Omnicampusの宿題タブから「第5回 畳み込みニューラルネットワーク (code)」を選択して**提出してください．pythonファイル自体の提出ではなく，「提出内容」の部分にコード全体をコピー&ペーストしてください．
      
- なお，採点は1で行い，2はコードの確認用として利用します（成績優秀者はコード内容を公開させていただくかもしれません）．コードの内容を変更した場合は，**1と2の両方を提出し直してください**．

### 評価方法

- 予測ラベルの`t_test`に対する精度 (Accuracy) で評価します．
- 即時採点しLeader Boardを更新します（採点スケジュールは別アナウンス）．
- 締切時の点数を最終的な評価とします．
"""

# 0) Drive mount & paths
from google.colab import drive
drive.mount('/content/drive', force_remount=True)
work_dir = 'drive/MyDrive/第5回'

# 1) Imports
import os, math, random, time
import numpy as np
import pandas as pd
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR
from torch.utils.data import Dataset, DataLoader, random_split
from torchvision import transforms

# ==========================================
# CIFAR-10 Colab 実行用（WRN-28-10 + クリーン・ファインチュン実装版）
# - Backbone: WideResNet-28-10 (dropout=0.3)
# - Recipe: RandAug + Mixup/CutMix + EMA + 終盤 Clean-FT（WD=0, Aug=OFF）
# - 推論: Flip + ±1px Shift + 多尺度(0.97,1.00,1.03) TTA
# ==========================================
import os, math, random, time, warnings, copy, gc
import numpy as np
import pandas as pd
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.multiprocessing as mp

from torch.optim.lr_scheduler import LambdaLR, CosineAnnealingLR, SequentialLR
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms

# ------- DataLoader周りの安定化 -------
try:
    mp.set_start_method('fork', force=True)  # Linux/ColabならforkでOK
except RuntimeError:
    pass

warnings.filterwarnings("ignore", message=r"The epoch parameter in `scheduler\.step\(\)`")

# 1) Repro & Device
def fix_seed(seed: int = 42):
    random.seed(seed); np.random.seed(seed)
    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = False
    torch.backends.cudnn.benchmark = True
fix_seed(42)

# NOTE: 事前セルで work_dir を定義していない場合はここで指定
assert 'work_dir' in globals(), "work_dir が未定義です（Driveマウント/pathsセルを先に実行してください）"

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device:', device)
try:
    torch.set_float32_matmul_precision("high")
except Exception:
    pass

# 2) Data load（課題配布の3配列のみ使用）
x_train = np.load(os.path.join(work_dir, 'data/x_train.npy'))
t_train = np.load(os.path.join(work_dir, 'data/t_train.npy'))
x_test  = np.load(os.path.join(work_dir, 'data/x_test.npy'))
assert x_train.ndim == 4 and x_train.shape[-1] in (1,3)
assert x_test.ndim  == 4 and x_test.shape[-1]  in (1,3)
NUM_CLASSES = 10

# 3) Dataset
class ImageDataset(Dataset):
    def __init__(self, x: np.ndarray, y: np.ndarray | None, transform):
        self.x = x
        self.y = None if y is None else y.astype(np.int64).reshape(-1)
        self.transform = transform
    def __len__(self): return self.x.shape[0]
    def __getitem__(self, idx):
        img = self.x[idx]
        if img.dtype != np.uint8:
            img = np.clip(img, 0, 255).astype(np.uint8)
        # PILが (H,W,1) を扱いにくい環境があるため squeeze
        if img.ndim == 3 and img.shape[2] == 1:
            img = img.squeeze(-1)
        img = Image.fromarray(img)
        img = self.transform(img) if self.transform else img
        if self.y is None:
            return img
        return img, torch.tensor(self.y[idx], dtype=torch.long)

# 4) channel mean/std を train から算出
def compute_channel_stats(arr_uint8: np.ndarray):
    x = arr_uint8.astype(np.float32) / 255.0
    mean = x.mean(axis=(0,1,2))
    std  = x.std(axis=(0,1,2))
    # もしグレースケールなら mean/std を3チャンネルへ複製
    if mean.shape == () or (hasattr(mean, "shape") and mean.shape == ()):
        mean = np.array([float(mean)]*3)
        std  = np.array([float(std)]*3)
    if mean.shape[0] == 1:
        mean = np.repeat(mean, 3)
        std  = np.repeat(std, 3)
    return mean.tolist(), std.tolist()

mean_c, std_c = compute_channel_stats(x_train.astype(np.uint8))
print("channel mean:", mean_c, "std:", std_c)

# 5) Transforms
randaug = transforms.RandAugment(num_ops=2, magnitude=10)  # 退火でMを変更（クリーンFTでは使わない）
transform_train = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    randaug,
    transforms.ToTensor(),
    transforms.Normalize(mean=mean_c, std=std_c),
    transforms.RandomErasing(p=0.25, scale=(0.02, 0.2), ratio=(0.3, 3.3)),
])
transform_eval = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=mean_c, std=std_c),
])

# 6) Split & Loader
N = x_train.shape[0]
val_size = 5000 if N >= 50000 else 3000
rng = np.random.RandomState(42)
perm = rng.permutation(N)
val_idx, trn_idx = perm[:val_size], perm[val_size:]

train_ds_aug   = ImageDataset(x_train[trn_idx], t_train[trn_idx], transform=transform_train)
train_ds_clean = ImageDataset(x_train[trn_idx], t_train[trn_idx], transform=transform_eval)  # クリーンFT用
val_ds         = ImageDataset(x_train[val_idx], t_train[val_idx], transform=transform_eval)
test_ds        = ImageDataset(x_test, None, transform=transform_eval)

# ★ WRN-28-10 は重いのでデフォは 128。VRAM余裕があれば 256 へ。
batch_size  = 128
num_workers_train = 2
num_workers_eval  = 0
pin_memory  = (device.type == 'cuda')
persistent  = False

train_loader_aug = DataLoader(
    train_ds_aug, batch_size=batch_size, shuffle=True,
    num_workers=num_workers_train, pin_memory=pin_memory,
    persistent_workers=persistent, drop_last=True
)
train_loader_clean = DataLoader(  # 終盤クリーンFT用
    train_ds_clean, batch_size=batch_size, shuffle=True,
    num_workers=num_workers_train, pin_memory=pin_memory,
    persistent_workers=persistent, drop_last=True
)
val_loader = DataLoader(
    val_ds, batch_size=batch_size, shuffle=False,
    num_workers=num_workers_eval, pin_memory=pin_memory,
    persistent_workers=False
)
test_loader = DataLoader(
    test_ds, batch_size=batch_size, shuffle=False,
    num_workers=num_workers_eval, pin_memory=pin_memory,
    persistent_workers=False
)

# 7) Model (WideResNet-28-10, CIFAR構成)
def conv3x3(in_planes, out_planes, stride=1):
    return nn.Conv2d(in_planes, out_planes, 3, stride=stride, padding=1, bias=False)

class WideBasic(nn.Module):
    def __init__(self, in_planes, planes, dropout_rate, stride=1):
        super().__init__()
        self.bn1 = nn.BatchNorm2d(in_planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = conv3x3(in_planes, planes, 1)
        self.bn2 = nn.BatchNorm2d(planes)
        self.dropout = nn.Dropout(p=dropout_rate, inplace=False) if dropout_rate > 0 else nn.Identity()
        self.conv2 = conv3x3(planes, planes, stride)

        self.shortcut = nn.Identity()
        if stride != 1 or in_planes != planes:
            self.shortcut = nn.Conv2d(in_planes, planes, kernel_size=1, stride=stride, bias=False)

    def forward(self, x):
        out = self.relu(self.bn1(x))
        out = self.conv1(out)
        out = self.relu(self.bn2(out))
        out = self.dropout(out)
        out = self.conv2(out)
        out = out + self.shortcut(x)
        return out

class WideResNet(nn.Module):
    # depth = 6n + 4, widen_factor=k
    def __init__(self, depth=28, widen_factor=10, dropout_rate=0.3, num_classes=10):
        super().__init__()
        assert (depth - 4) % 6 == 0, "WRN depth must be 6n+4"
        n = (depth - 4) // 6
        k = widen_factor

        widths = [16, 16*k, 32*k, 64*k]
        self.in_planes = widths[0]

        self.conv1 = conv3x3(3, widths[0], 1)
        self.block1 = self._make_group(widths[1], n, dropout_rate, stride=1)
        self.block2 = self._make_group(widths[2], n, dropout_rate, stride=2)
        self.block3 = self._make_group(widths[3], n, dropout_rate, stride=2)
        self.bn = nn.BatchNorm2d(widths[3])
        self.relu = nn.ReLU(inplace=True)
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(widths[3], num_classes)

        # Init
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
            elif isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')
                if m.bias is not None: nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm2d):
                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)

    def _make_group(self, planes, num_blocks, dropout_rate, stride):
        strides = [stride] + [1]*(num_blocks-1)
        layers = []
        for s in strides:
            layers.append(WideBasic(self.in_planes, planes, dropout_rate, stride=s))
            self.in_planes = planes
        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.relu(self.bn(x))
        x = self.pool(x).flatten(1)
        x = self.fc(x)
        return x

model = WideResNet(depth=28, widen_factor=10, dropout_rate=0.3, num_classes=NUM_CLASSES).to(device)
if device.type == 'cuda':
    model = model.to(memory_format=torch.channels_last)

# 8) Optimizer (BN/biasはWD除外)
def wd_groups(model, wd):
    decay, no_decay = [], []
    for m in model.modules():
        if isinstance(m, (nn.Conv2d, nn.Linear)):
            decay.append(m.weight)
            if m.bias is not None: no_decay.append(m.bias)
        elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm, nn.LayerNorm)):
            if m.weight is not None: no_decay.append(m.weight)
            if m.bias  is not None: no_decay.append(m.bias)
    return [{"params": decay, "weight_decay": wd},
            {"params": no_decay, "weight_decay": 0.0}]

base_lr = 0.1
weight_decay = 5e-4
epochs = 160  # 0.97狙い

base_optimizer = optim.SGD(wd_groups(model, weight_decay),
                           lr=base_lr, momentum=0.9, nesterov=True)

# ---- SAM（必要なら True に）
USE_SAM = False
class SAM(torch.optim.Optimizer):
    def __init__(self, optimizer, rho=0.05):
        self.optimizer = optimizer
        self.param_groups = self.optimizer.param_groups
        self.state = self.optimizer.state
        self.rho = rho
    @torch.no_grad()
    def _grad_norm(self):
        device = self.param_groups[0]["params"][0].device
        norms = []
        for g in self.param_groups:
            for p in g["params"]:
                if p.grad is None: continue
                norms.append(torch.norm(p.grad.detach(), p=2))
        if not norms: return torch.tensor(0., device=device)
        return torch.norm(torch.stack(norms), p=2)
    @torch.no_grad()
    def first_step(self):
        scale = self.rho / (self._grad_norm() + 1e-12)
        for g in self.param_groups:
            for p in g["params"]:
                if p.grad is None: continue
                e_w = p.grad * scale
                p.add_(e_w); self.state[p]["e_w"] = e_w
    @torch.no_grad()
    def second_step(self):
        for g in self.param_groups:
            for p in g["params"]:
                if p.grad is None: continue
                p.sub_(self.state[p]["e_w"])
        self.optimizer.step()
    def zero_grad(self, set_to_none=True):
        self.optimizer.zero_grad(set_to_none=set_to_none)

optimizer = SAM(base_optimizer, rho=0.05) if USE_SAM else base_optimizer

# 9) LR schedule (Warmup + Cosine) ※ep145以降は手動LR固定へ切替
warmup_epochs = 5
def lr_lambda_warmup(epoch): return (epoch + 1) / warmup_epochs
scheduler_warmup = LambdaLR(optimizer, lr_lambda=lr_lambda_warmup)
scheduler_cosine  = CosineAnnealingLR(optimizer, T_max=(epochs - warmup_epochs),
                                      eta_min=base_lr * 1e-4)
scheduler = SequentialLR(optimizer, [scheduler_warmup, scheduler_cosine], milestones=[warmup_epochs])

# 10) Loss
ce       = nn.CrossEntropyLoss().to(device)
ce_ls01  = nn.CrossEntropyLoss(label_smoothing=0.1).to(device)  # eval用

# 11) Mixup & CutMix（前半のみ/終盤はOFF）
def mixup_data(x, y, alpha=0.4):
    lam = np.random.beta(alpha, alpha)
    index = torch.randperm(x.size(0), device=x.device)
    mixed_x = lam * x + (1 - lam) * x[index]
    return mixed_x, y, y[index], lam

def rand_bbox(W, H, lam):
    cut_rat = math.sqrt(1. - lam)
    cut_w, cut_h = int(W * cut_rat), int(H * cut_rat)
    cx, cy = np.random.randint(W), np.random.randint(H)
    x1, y1 = np.clip(cx - cut_w // 2, 0, W), np.clip(cy - cut_h // 2, 0, H)
    x2, y2 = np.clip(cx + cut_w // 2, 0, W), np.clip(cy + cut_h // 2, 0, H)
    return x1, y1, x2, y2

def cutmix_data(x, y, alpha=0.4):
    lam = np.random.beta(alpha, alpha)
    index = torch.randperm(x.size(0), device=x.device)
    y_a, y_b = y, y[index]
    _, _, H, W = x.size()
    x1, y1, x2, y2 = rand_bbox(W, H, lam)
    x[:, :, y1:y2, x1:x2] = x[index, :, y1:y2, x1:x2]
    lam = 1 - ((x2 - x1) * (y2 - y1) / (W * H))
    return x, y_a, y_b, lam

def mix_criterion(logits, y_a, y_b, lam):
    return lam * ce(logits, y_a) + (1 - lam) * ce(logits, y_b)

# 12) EMA
class ModelEMA:
    def __init__(self, model, decay=0.999):
        self.ema = copy.deepcopy(model).eval()
        for p in self.ema.parameters():
            p.requires_grad_(False)
        self.decay = decay
    @torch.no_grad()
    def update(self, model):
        d = self.decay
        msd = model.state_dict()
        for k, v in self.ema.state_dict().items():
            if v.dtype.is_floating_point:
                v.copy_(v * d + msd[k] * (1. - d))
            else:
                v.copy_(msd[k])

ema = ModelEMA(model, decay=0.999)
def ema_decay_by_epoch(ep):
    if ep < 60:   return 0.999
    elif ep < 100:return 0.9995
    else:         return 0.9998

# 13) AMP
scaler = torch.amp.GradScaler(enabled=(device.type=='cuda'))

# 14) 退火ユーティリティ（終盤は不使用）
def set_randaug_M(compose, M):
    for t in getattr(compose, "transforms", []):
        if isinstance(t, transforms.RandAugment):
            t.magnitude = int(M)

def set_random_erasing_p(compose, p):
    for t in getattr(compose, "transforms", []):
        if isinstance(t, transforms.RandomErasing):
            t.p = float(p)

# WDの動的切替（クリーンFTで0にする）
def set_weight_decay(optim_obj, wd: float):
    for g in optim_obj.param_groups:
        g['weight_decay'] = float(wd)

# 15) Train / Eval
FT_START = 145             # クリーン・ファインチュン開始epoch
FT_LR1 = 1e-3              # ep145〜154
FT_LR2 = 1e-4              # ep155〜159
FT_PHASE2_START = 155

def set_optimizer_lr(optim_obj, lr):
    for g in optim_obj.param_groups:
        g['lr'] = float(lr)

def run_epoch_train(loader, epoch: int, clean: bool = False):
    model.train(True)
    total, correct, loss_sum = 0, 0, 0.0

    if not clean:
        # RandAug退火（60→100: 10→6）
        if 60 <= epoch <= 100:
            M = np.interp(epoch, [60, 100], [10, 6])
            set_randaug_M(transform_train, M)
        # RandomErasing退火（80→160: 0.25→~0）
        re_p = 0.25 * max(0.0, 1.0 - max(0, epoch - 80) / 80.0)
        set_random_erasing_p(transform_train, re_p)
        alpha = 0.4 if epoch < 80 else 0.2
    else:
        # クリーンFT中は一切の拡張・混合を行わない
        re_p = 0.0
        alpha = None

    for x, y in loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        if device.type == 'cuda':
            x = x.to(memory_format=torch.channels_last)

        if not clean:
            # 通常フェーズ：Mixup or CutMix
            if np.random.rand() < 0.5:
                x_in, y_a, y_b, lam = mixup_data(x, y, alpha)
            else:
                x_in, y_a, y_b, lam = cutmix_data(x.clone(), y, alpha)
            with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
                logits = model(x_in)
                loss = mix_criterion(logits, y_a, y_b, lam)
        else:
            # クリーンFT：そのままCE
            x_in = x
            with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
                logits = model(x_in)
                loss = ce(logits, y)

        if USE_SAM and not clean:
            # SAMは通常フェーズのみ（必要なら拡張可）
            scaler.scale(loss).backward()
            scaler.unscale_(base_optimizer)
            optimizer.first_step()
            optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
                logits2 = model(x_in)
                if not clean:
                    loss2 = mix_criterion(logits2, y_a, y_b, lam)
                else:
                    loss2 = ce(logits2, y)
            scaler.scale(loss2).backward()
            scaler.unscale_(base_optimizer)
            optimizer.second_step()
            scaler.update()
            logits_use = logits2
        else:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad(set_to_none=True)
            logits_use = logits

        ema.decay = ema_decay_by_epoch(epoch)
        ema.update(model)

        # 学習中の簡易精度表示
        if not clean:
            pred = logits_use.argmax(1)
            correct += (pred == y_a).sum().item()
            total   += y_a.size(0)
        else:
            pred = logits_use.argmax(1)
            correct += (pred == y).sum().item()
            total   += y.size(0)

        loss_sum += loss.item() * (y.size(0))

    gc.collect()
    return loss_sum/total, correct/total, re_p

@torch.no_grad()
def evaluate(loader):
    model_for_eval = ema.ema
    model_for_eval.eval()
    total, correct, loss_sum = 0, 0, 0.0
    for x, y in loader:
        x = x.to(device, non_blocking=True)
        y = y.to(device, non_blocking=True)
        if device.type == 'cuda':
            x = x.to(memory_format=torch.channels_last)
        with torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
            logits = model_for_eval(x)
            loss = ce_ls01(logits, y)  # evalはLS0.1
        pred = logits.argmax(1)
        correct += (pred == y).sum().item()
        total   += y.size(0)
        loss_sum += loss.item() * y.size(0)
    gc.collect()
    return loss_sum/total, correct/total

# 16) Train loop（ep145からクリーンFT + 手動LR固定 + WD=0）
best_val = 0.0
for epoch in range(epochs):
    t0 = time.time()

    clean_mode = (epoch >= FT_START)
    loader = train_loader_clean if clean_mode else train_loader_aug

    # 手動LR/WD切替（ep145で lr=1e-3 & WD=0、ep155で lr=1e-4）
    if clean_mode:
        if epoch == FT_START:
            set_optimizer_lr(optimizer, FT_LR1)
            set_weight_decay(optimizer, 0.0)
        elif epoch == FT_PHASE2_START:
            set_optimizer_lr(optimizer, FT_LR2)

    train_loss, train_acc, re_p = run_epoch_train(loader, epoch, clean=clean_mode)
    val_loss,   val_acc         = evaluate(val_loader)

    # スケジューラ：クリーンFTに入ったら停止（手動LR）
    if epoch < FT_START:
        scheduler.step()

    best_val = max(best_val, val_acc)
    dt = time.time() - t0
    lr_now = optimizer.param_groups[0]['lr']

    mode_str = "CLEAN" if clean_mode else "AUG"
    print(f"EPOCH {epoch:03d}/{epochs-1} | "
          f"Train[{mode_str}] [Loss {train_loss:.4f}, Acc~ {train_acc:.4f}] | "
          f"Valid(EMA) [Loss {val_loss:.4f}, Acc {val_acc:.4f}] | "
          f"Best {best_val:.4f} | lr {lr_now:.5f} | RE p={re_p:.3f} | {dt:.1f}s")

# 17) 推論TTA：Flip + 多尺度(0.97,1.00,1.03) + ±1px シフト
@torch.no_grad()
def logits_tta_scale_shift_flip(model, x):
    # x: [B,C,H,W]
    scales = [0.97, 1.00, 1.03]
    shifts = [(0,0),(1,0),(-1,0),(0,1),(0,-1),(1,1),(1,-1),(-1,1),(-1,-1)]
    outs = []
    B, C, H, W = x.shape
    grid_y, grid_x = torch.meshgrid(
        torch.linspace(-1, 1, H, device=x.device),
        torch.linspace(-1, 1, W, device=x.device),
        indexing='ij'
    )
    base_grid = torch.stack((grid_x, grid_y), dim=-1).unsqueeze(0).repeat(B,1,1,1)  # [B,H,W,2]
    for flip in [False, True]:
        xx = torch.flip(x, dims=[-1]) if flip else x
        for s in scales:
            # 拡大縮小は grid を 1/s 倍（ズームイン: s>1 -> 1/s で中心拡大）
            gx_s = base_grid[...,0] / s
            gy_s = base_grid[...,1] / s
            for dx, dy in shifts:
                gx = gx_s - dx * (2.0 / W)
                gy = gy_s - dy * (2.0 / H)
                grid = torch.stack((gx, gy), dim=-1)
                xs = F.grid_sample(xx, grid, mode='bilinear', padding_mode='zeros', align_corners=True)
                outs.append(model(xs))
    return torch.stack(outs, dim=0).mean(0)

# 18) Test inference & save
model_for_test = ema.ema.eval()
preds = []
with torch.no_grad(), torch.amp.autocast(device_type='cuda', enabled=(device.type=='cuda')):
    for x in test_loader:
        x = x.to(device, non_blocking=True)
        if device.type == 'cuda':
            x = x.to(memory_format=torch.channels_last)
        logits = logits_tta_scale_shift_flip(model_for_test, x)
        preds.extend(logits.argmax(1).tolist())

out_path = os.path.join(work_dir, 'submission.csv')
pd.Series(preds, name='label').to_csv(out_path, header=True, index_label='id')
print("Saved:", out_path)